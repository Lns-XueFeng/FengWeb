# Python爬虫与逆向

## 何谓网络爬虫？

网络爬虫，又被称为网页蜘蛛，网络机器人，在FOAF社区中间，更经常的称为网页追逐者，是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本，另外一些不常使用的名字还有蚂蚁、自动索引、模拟程序或者蠕虫。

大部分爬虫都是按“发送请求——获得页面——解析页面——抽取并储存内容”这样的流程来进行，这其实也是模拟了我们使用浏览器获取网页信息的过程。

简单来讲，爬虫就是一个探测机器，它的基本操作就是模拟人的行为去各个网站溜达，点点按钮，查查数据，或者把看到的信息背回来。就像一只虫子在一幢楼里不知疲倦地爬来爬去。

可以简单地想象：每个爬虫都是你的“分身”。就像孙悟空拔了一撮汗毛，吹出一堆猴子一样。

我们每天使用的搜索引擎，其实就是利用了这种爬虫技术：每天放出无数爬虫到各个网站，把他们的信息抓回来，然后化好淡妆排着小队等你来检索。

## 爬虫能干什么？

往大了说爬虫可以做搜索引擎，例如百度，搜狗，谷歌等，这一类爬虫为通用爬虫，它们通常爬取的一般都是千万级别，千亿级别的信息。写这类爬虫的时候通常需要考虑爬取效率，占用带宽，内存等硬件资源。

往小了说爬虫可以是一个随手写出来的自动化脚本，去自动化的爬取某些我们需要的信息。这类信息通常数量不大，但是爬取难度较高，往往是一些网站或app中比较有价值的信息。这里我们需要知道的是爬虫不等于渗透，爬虫是爬取为们所能看到的东西，可见即可爬。而渗透则是直接冻了人家的数据库等，因此一般情况下爬虫并不违法。记住是一般情况下，因为当你爬取了个人隐私或者是爬取了对方网站的信息并做了盈利竞争，那么你离进去就不远了。就比如之前厦门有一个公司爬取某个租房网的全部租房信息做成app进行盈利，最后以不正当竞争和非法入侵计算机罪判刑了，当时还是给我们爬虫圈子造成了不小的轰动。

关于爬虫还能干什么的无限可能需要你自己去根据自身需要进行发现，加油吧少年郎，一个崭新的世界在朝你敞开！

## 爬虫需学哪些知识？

爬虫所需要的知识比较杂比较散！

首先你需要掌握HTML，CSS，JavaScript前端三件套，这里并不是说一定要你一开始就完全掌握它们（之前就会的最好），而是先最起码的了解它们，然后在再后续的爬虫中进行完善。比如HTML和CSS越熟悉，爬虫时寻找信息，定位信息越快速，而熟练掌握JavaScript则是后续学习JS逆向的基础。简单来说，我们需要爬取的信息在一张网上，那张网就是由上述三件套构建而成！

其次就是懂得HTTP/HTTPS协议，网页原理。服务器，网页端和客户端之间的关系。这个尤为重要，因为只有你了解上述原理之后才能真正的知道爬虫干了什么，才可以开始尝试写爬虫脚本。

一个很重要的就是熟练使用网页开发者工具。进行抓包，找数据包，了解ajax原理，了解各种请求参数，然后进行伪装或是知道需要破解那些加密参数。其中，Fiddler，Postman，网页开发工具均可。

接着就是挑选一个可以实现HTTP协议的库，比如urlib，requests，aiohttp，httpx等，本笔记会讲述的是requests和aiohttp。通过学习它们我们就可以构建出一个个爬虫去访问/浏览网站。

之后就是解析网页，找到我们所需要的信息。BeautifulSoup4，Xpath，正则等库可进行解析。另外还需要会json，因为爬取网页时会遇到大量的json数据包。

最后是存储，存储爬取的数据，txt，csv，json，数据库等均可，看情况来进行选择。

这只是大概，这其中还会涉及selenium自动化测试工具，爬虫框架，各种七七八八的库来解决问题。

因此在爬取网页的过程中我们也需要不断的查博客来解决我们所遇到的各种奇奇怪怪的问题，只有在你爬取了足够多的网站，踩了数不清的坑之后你的爬虫技术才算还可以，接着就是看你要通过框架/构建框架爬取海量数据，还是聚焦于逆向爬虫。

在基本掌握了上述技术之后，就是朝着大型爬虫->分布式爬虫，逆向爬虫->应对反爬解决加密，app爬虫，智能化爬虫钻研了。

## 由简入繁 由繁入简

我们来写一个超级简单的爬虫作为引子，来引出一系列内容。在开始之前我们需要安装requests --- 命令为：pip install requests.

```python
import requests

# 简单的请求
res = requests.get(url='https://www.baidu.com')
print(res.status_code)

>>>200
```

观察上述代码，我们可以知道其实核心代码就两行，一个是第四行对www.baidu.com进行请求并返回响应，另一个是第五行打印出响应状态码。如果是200则请求成功！

读到这里请联想我们打开百度的时候是如何操作的？我给出的步骤，假设我的浏览器已经打开并处于空表浅栏。

我会在网址栏输入http://www.baidu.com然后回车，此时不出意外的话浏览器会返回此网址对应网站的响应，即百度搜索页面。

这一人为步骤用代码实现就变成了上面代码的模样，我使用requests库来进行请求，我只需要规定好需要请求的网址，requests会自动帮助我们进行请求。

如果细心一些的话就会发现requests后面跟了一个get方法，在requests中最常用的有两种请求方法，一个是get方法另一个是post方法。对于用那个方法这就牵扯到你需要请求的网址在开发者工具中显示的是get还是post。因此在开始爬虫之前你需要掌握如何使用网页开发者工具。

因此在开始学习爬虫之前之前请务必学习HTTP原理，了解HTML，CSS，JavaScript和掌握网页开发者工具的使用。

## 网页是如何形成的？

程序员开发出网站并将网站托管在服务器上，然后映射域名，备案网站至发布到万维网上。然后仅仅通过一串链接即可访问其网站资源，这一过程中是客户端发出请求，服务器接收并处理请求，最后服务器返回响应至客户端。当服务器将响应返回客户端后，浏览器便开始着手渲染网页的形成。通过观察网页开发者工具中的瀑布流，会发现最先加载完毕的数据包为文档类型中的html，这个就是网页的大致框架，里面基本包含网页的布局，主题，主体内容......但不会包含图片（一般是图片链接），视频等。然后浏览器根据css，javascript对html页面进行渲染，该加载出图片的加载出图片，该有字体样式的加载出字体样式等等，最终全部渲染完毕，网页全部的展示给你。

通过上面的描述，我们引出另外一个很重要的点，即，静态页面和动态页面。

静态页面通常比较简单，基本就是html和css，打开速度快，基本上是页面显示的是啥源代码就是啥，因此利用requests模拟请求时直接根据源代码进行定位即可。

动态页面则比较复杂且广泛应用，其主要特点是网页源代码是网站的基本框架，但是主体内容基本不在此源代码中，而是在后续一个个在请求响应出来的，因此在应对动态页面的时候通常是抓数据包，找到所需数据的那个请求，然后单独请求那个数据包得到数据。

比如我们通常在浏览页面时会点击展开（展开我们想看的内容），当点击后浏览器就会处理我们的请求，向服务器请求此数据的响应，当服务器觉着这一次正常请求后便会返回响应，浏览器根据响应进行渲染展开下面的数据，你便看到了下面的内容。如果这一过程你打开了网页开发者工具便会发现点击展开并得到数据之后网页开发者工具中会有一个数据包出现，你想看的内容就在此数据包中，只是它的形式没有网页中那么好看而已......

另外你可能觉得好奇，网页是如何知道自己是谁的？其实是这样的，http并不存在记忆这一说，也就是说你没发送一次请求对于服务器而言都是一个个单独的请求，它们之间并没有关系，那是什么让服务器有时候能知道它们之间是有关系的呢？

这就是session和cookie的功劳！

## Session和Cookie

**什么是 Session**

Session 代表着服务器和客户端一次会话的过程。Session 对象存储特定用户会话所需的属性及配置信息。这样，当用户在应用程序的 Web 页之间跳转时，存储在 Session 对象中的变量将不会丢失，而是在整个用户会话中一直存在下去。当客户端关闭会话，或者 Session 超时失效时会话结束。

**什么是 Cookie**

HTTP Cookie（也叫 Web Cookie或浏览器 Cookie）是服务器发送到用户浏览器并保存在本地的一小块数据，它会在浏览器下次向同一服务器再发起请求时被携带并发送到服务器上。通常，它用于告知服务端两个请求是否来自同一浏览器，如保持用户的登录状态。Cookie 使基于无状态的 HTTP 协议记录稳定的状态信息成为了可能。

Cookie 主要用于以下三个方面：

会话状态管理（如用户登录状态、购物车、游戏分数或其它需要记录的信息）个性化设置（如用户自定义设置、主题等）浏览器行为跟踪（如跟踪分析用户行为等）Cookie和Session的区别

cookie和session的共同之处在于：cookie和session都是用来跟踪浏览器用户身份的会话方式。

**区别在于：**

1. 由于HTTP协议是无状态的协议，所以服务端需要记录用户的状态时，就需要用某种机制来识别具体的用户，这个机制就是Session，典型的场景比如购物车，当你点击下单按钮时，由于HTTP协议无状态，所以并不知道是哪个用户操作的，所以服务端要为特定的用户创建了特定的Session，用于标识这个用户，并且跟踪用户，这样才知道购物车里面有几本书。这个Session是保存在服务端的，有一个唯一标识。在服务端保存Session的方法很多，内存、数据库、文件都有。

集群的时候也要考虑Session的转移，在大型的网站，一般会有专门的Session服务器集群，用来保存用户会话，这个时候 Session 信息都是放在内存的，使用一些缓存服务比如Memcached之类的来放 Session。

2. 思考一下服务端如何识别特定的客户？这个时候Cookie就登场了。每次HTTP请求的时候，客户端都会发送相应的Cookie信息到服务端。实际上大多数的应用都是用 Cookie 来实现Session跟踪的，第一次创建Session的时候，服务端会在HTTP协议中告诉客户端，需要在 Cookie 里面记录一个Session ID，以后每次请求把这个会话ID发送到服务器，我就知道你是谁了。

有人问，如果客户端的浏览器禁用了 Cookie 怎么办？一般这种情况下，会使用一种叫做URL重写的技术来进行会话跟踪，即每次HTTP交互，URL后面都会被附加上一个诸如 sid=xxxxx 这样的参数，服务端据此来识别用户。

3. Cookie其实还可以用在一些方便用户的场景下，设想你某次登陆过一个网站，下次登录的时候不想再次输入账号了，怎么办？这个信息可以写到Cookie里面，访问网站的时候，网站页面的脚本可以读取这个信息，就自动帮你把用户名给填了，能够方便一下用户。这也是Cookie名称的由来，给用户的一点甜头。

所以，总结一下：

**Session是在服务端保存的一个数据结构，用来跟踪用户的状态，这个数据可以保存在集群、数据库、文件中；**

**Cookie是客户端保存用户信息的一种机制，用来记录用户的一些信息，也是实现Session的一种方式。**

## Requests请求库

**1、requests库的七个主要方法:**

- ​    requests.request() 构造一个请求，支持以下各种方法
- ​    requests.get()   获取html的主要方法
- ​    requests.head()    获取html头部信息的主要方法
- ​    requests.post()    向html网页提交post请求的方法
- ​    requests.put()   向html网页提交put请求的方法
- ​    requests.patch()  向html提交局部修改的请求
- ​    requests.delete() 向html提交删除请求



**2.requests.get():**

**r=requests.get(url,params,**kwargs):**

**url: 需要爬取的网站地址**

**params: 翻译过来就是参数， url中的额外参数，字典或者字节流格式，可选**

**kwargs : 12个控制访问的参数:**

- params:字典或字节序列,作为参数增加到url中(get方法时用,以字典形式传入关键字参数)
- data：字典,字节序或文件对象,重点作为向服务器提供或提交资源是提交,作为request的内容,与params不同的是,data提交的数据并不放在url链接里,而是放在url链接对应位置的地方作为数据来存储,它也可以接受一个字符串对象(post方法提交表单)
- headers:可以用来模拟任何我们想模拟的浏览器来对url发起访问
- timeout:用于设定超时时间,单位为秒,当发起一个get请求时可以设置一个timeout时间,如果在timeout时间内请求内容没有返回,将产生一个timeout的异常
- proxies:字典,用来设置访问代理服务器
- cookies:字典或CookieJar,指的是从http中解析cookie
- auth:元组,用来支持http认证功能
- files:字典,是用来向服务器传输文件时使用的字段
- allow_redirects:开关,表示是否允许对url进行重定向,默认为True。
- stream: 开关,指是否对获取内容进行立即下载,默认为True。
- verify：开关,用于认证SSL证书， 默认为True。
- cert:用于设置保存本地SSL证书路径



**3.requests.post():**

**r=requests.post(url,kwargs):kwargs:见上述**



**4.requests.request():**

requests.request(）支持其他所有的方法。

requests.request(method，url,**kwargs)

- method: “GET”、”HEAD”、”POST”、”PUT”、”PATCH”等等
- url: 请求的网址
- **kwargs: 控制访问的参数

  

**5.requests.Session():**

```python
#创建Session对象

s = requests.Session()

#利用Session对象来对需要进行用户验证的网站发起请求
data={
    'username': 'byhy',
    'password': '88888888'
}
response = s.post("http://127.0.0.1/api/mgr/signin",data=data)

# 通过 Session 对象 发送请求
params={
    'action'  : 'list_customer',
    'pagesize' : 10,
    'pagenum'  : 1,
    'keywords' : '',
}
response = s.get("http://127.0.0.1/api/mgr/customers",params=params)
```



  **6.response属性:**

- r.status_code   http请求的返回状态，若为200则表示请求成功
- r.text       http响应内容的字符串形式，即返回的页面内容
- r.encoding     从http header 中猜测的相应内容编码方式
- r.apparent_encoding  从内容中分析出的响应内容编码方式（备选编码方式）
- r.content     http响应内容的二进制形式

总结：

## 解析神器：Xpath

 **xpath解析原理:**

- 实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象当中
- 调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获

 **环境的安装：**

- pip install lxml



 **如何实例化一个etree对象：**

```python
from lxml import etree

# 将本地的html文档中的源码数据加载到etree对象中：
etree.parse(filepath)

# 可以将从互联网上获取的源码数据加载到该对象当中:
etree.HTML('page_text')
```



 **xpath表达式：**

xpath('xpath表达式')

- /:表示的是从根节点开始定位。表示的是一个层级。
- //:表示的是多个层级。可以表示从任意位置开始定位。
- 属性定位://div[@class="属性名"] == //tag[@attrName='attrvalue']
- 索引定位://div[@class="属性名"]/p[3]--这个地方索引是从1开始的，不是0哦！
- /text() 获取的是标签中直系的文本内容
- //text() 获取的是标签中的非直系的文本内容，就是可以继续跨越层级去获取(标签内所有的文本内容)
- @attrName ------例如：img/@src------取属性是通过@来特指的



 **具体的xpath定位:**

​    from lxml import etree

​    **实例化一个etree对象并且将被解析的源码数据传入该对象**

​    tree = etree.parse('test.html')

​    r = tree.xpath('/html/body/div')---一级一级的往里面去套，千层套娃

​    r = tree.xpath('/html//div')---这里//直接跨过了body这一级，//表示跨越多层

​    r = tree.xpath('//div')---这里//前面啥也没有，是指从根节点下(这边的根节点是html)定位所有的div标签

​    r = tree.xpath('//li[7]//text()')---这里表示：从整个源码中取第七个li，并且返回文本数据列表



​    r = tree.xpath('//div[@class="song"]')---这里是定位了众多div里class="song"的div，属性的定位用@......

​    r = tree.xpath('//div[@class="tang"]//li[5]/a/text()')[0]---这里表示：定位到第五个li标签中的a标签并且把文本数据取出，最后将返回列表的第一个元素传给对象

​    r = tree.xpath('//div[@class="tang"]//text()')---这里表示从tang这个div中返回所有的文本数据

​    r = tree.xpath('//div[@class="song"]/img/@src')---这里表示定位song这个div中的img标签中属性为src的图片地址



**注意:定位比较外层的属性时，例如定位某个特定的div---xpath('//div[@class="属性名"'),而定位比较里面的标签里面的属性的时侯，例如定位某个图片地址---xpath('//div//img/@src')**

**它们的不同之处就是：前者无/而是[@class="属性名"](不止有class还有其他的，这里以这个为例)，而后者是/标签名/@属性名，无[]而有/**



**在实例时会遇到的问题：**

**1.在进行for循环遍历内层标签时，xpath解析表达式最开始需要加上./，./表示：当前标签下。**

例如：

for li in li_list:

​	data = li.xpath('./a/text()')此时./表示的就是在当前li标签下

**2.解决输出中文乱码的情况：**

一般情况下在遇到中文乱码的情况可以直接在响应哪里使用response.encoding = 'utf-8'(其中response是解析返回的html数据)

在utf-8也无法解决的时候，可以在有中文乱码的地方写上.encode('iso-8859-1').decode('gbk')即可---.encode前面写乱码的数据的那个变量

**3.想一下子xpath解析两个或者多个对象，并且他们的层级结构不一样时：**

可以使用逻辑值(我的叫法，xpath里面的东西)，|表示‘或’
